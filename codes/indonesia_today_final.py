# -*- coding: utf-8 -*-
"""indonesia_today_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T-3U6iXHiL-QNQTfEChg03ge3X44V0PW
"""

# -*- coding: utf-8 -*-
"""indonesia_today_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17FAWyj_S4NRUCQgBGV_NawgU33z2WgcC
"""

#!/usr/bin/env python
# coding: utf-8

# In[ ]:


from newspaper import Article
from bs4 import BeautifulSoup
import requests
from dateutil import parser
import time
import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from newspaper import Article

# 검색할 기업 리스트
In_Top30_Name_list = [
    "HANA", "GS", "LG", "SK", "DAELIM", "DAEWOO", "DOOSAN",
    "LOTTE", "SAMSUNG", "SHINHAN", "SSANGYONG", "GOLFZON", "Pantos",
    "IBK", "KT&G", "POSCO", "HANHWA", "HANJIN", "HYUNDAI", "KOLON",
    "ASIANA AIRLINE", "KOREAN AIR", "LS", "KUMKANG", "CJ" , "lock&lock", "hansoll"
]

data = [
    ["정부부처", "인도네시아 투자부", "https://www7.bkpm.go.id/"],
    ["정부부처", "인도네시아 검찰청", "https://www.kejaksaan.go.id/"],
    ["정부부처", "인도네시아 Supreme Audit Board", "https://www.bpk.go.id/"],
    ["정부부처", "인도네시아 중앙은행", "https://www.bi.go.id/id/default.aspx"],
    ["정부부처", "인도네시아 Constitution Court", "https://www.mkri.id/"],
    ["정부부처", "인도네시아 상공회의소", "https://kadin.id/"],
    ["정부부처", "인도네시아 국가사무처", "http://setneg.go.id/"],
    ["정부부처", "인도네시아 House of Representative", "https://www.dpd.go.id/"],
    ["정부부처", "인도네시아 지식재산청", "http://dgip.go.id/"],
    ["정부부처", "인도네시아 경영자협회", "http://apindo.or.id/"],
    ["정부부처", "인도네시아 대통령궁", "http://presidenri.go.id/"],
    ["정부부처", "인도네시아 지역대표의회(DPD)", "http://dpd.go.id/"],
    ["정부부처", "인도네시아 국민평의회(MPR)", "http://mpr.go.id/"],
    ["정부부처", "인도네시아 증권거래소", "https://www.idnfinancials.com/id/"],
    ["정부부처", "인도네시아 전력공사", "https://www.pln.co.id/"],
    ["정부부처", "인도네시아 경제조정부", "https://www.ekon.go.id/"],
    ["정부부처", "인도네시아 보건의료부", "https://www.kemkes.go.id/id/home"],
    ["정부부처", "인도네시아 식약청", "https://www.pom.go.id/"],
    ["정부부처", "인도네시아 외교부", "https://kemlu.go.id/portal/id"],
    ["정부부처", "인도네시아 내무부", "https://setkab.go.id/"],
    ["정부부처", "인도네시아 국방부", "https://www.kemhan.go.id/"],
    ["정부부처", "인도네시아 법무부", "https://centralauthority.kemenkumham.go.id/"],
    ["정부부처", "인도네시아 무역부", "https://representative.kemendag.go.id/"],
    ["정부부처", "인도네시아 공공사업주택부", "https://pu.go.id/"],
    ["정부부처", "인도네시아 National Institute of Public Administration", "https://lan.go.id/"],
    ["정부부처", "인도네시아 Agency for Nuclear Energy Control", "https://www.bapeten.go.id/"],
    ["정부부처", "인도네시아 교통부", "https://www.dephub.go.id/"],
    ["정부부처", "인도네시아 노동부", "https://kemnaker.go.id/"],
    ["정부부처", "인도네시아 정보통신부", "https://www.kominfo.go.id/"],
    ["정부부처", "인도네시아 관광창조경제부", "https://www.kemenparekraf.go.id/"],
    ["정부부처", "인도네시아 에너지광물자원부", "https://www.menpan.go.id/site/"],
    ["정부부처", "인도네시아 교육문화연구기술부", "https://www.kemdikbud.go.id/"],
    ["정부부처", "인도네시아 국가개발계획부", "https://www.bappenas.go.id/"],
    ["정부부처", "인도네시아 국영기업부", "https://bumn.go.id/"],
    ["정부부처", "인도네시아 국가자산관리청", "https://lman.kemenkeu.go.id/"],
    ["정부부처", "인도네시아 재무부", "https://www.kemenkeu.go.id/"],
    ["정부부처", "인도네시아 자카르타시사이트", "https://jakarta.go.id/"],
    ["정부부처", "인도네시아 수라바야시사이트", "https://surabaya.go.id/"],
    ["정부부처", "인도네시아 메단시사이트", "https://portal.pemkomedan.go.id/"],
    ["정부부처", "인도네시아 자카르타 챔버 오브 커머스", "https://kadinjakarta.or.id/"],

    ["방산업체", "인도네시아 해군조선소", "https://www.pal.co.id/"],
    ["방산업체", "인도네시아 항공우주제조기업", "https://www.indonesian-aerospace.com/"],
    ["방산업체", "인도네시아 지상체계제조기업", "https://pindad.com/"],
    ["방산업체", "인도네시아 폭발물 제조기업", "https://dahana.id/"],
    ["방산업체", "인도네시아 국방전자 장비기업", "https://www.len.co.id/"],
    ["방산업체", "인도네시아 국영철도기업", "https://www.kai.id/"],
    ["방산업체", "인도네시아 국영 니켈 및 금 채굴 회사", "https://antam.com/en"],
    ["방산업체", "인도네시아 국영 배터리 홀딩", "https://www.indonesiabatterycorp.com/id"],
    ["방산업체", "인도네시아 국영육상운송회사", "https://damri.co.id/"],
    ["방산업체", "인도네시아 국영석유회사", "https://www.pertamina.com/"],

    ["언론", "인도네시아 KOMPAS", "http://www.kompas.com/"],
    ["언론", "인도네시아 자카르타포스트", "https://www.thejakartapost.com/"],
    ["언론", "인도네시아 국영뉴스기관", "https://www.antaranews.com/"], #영자신문
    ["언론", "인도네시아 자카르타글로브", "https://jakartaglobe.id/"] #영자신문
]

def initialize_chrome_driver():
  # Chrome 옵션 설정 : USER_AGENT는 알아서 수정
  USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
  chrome_options = Options()
  chrome_options.page_load_strategy = 'normal'  # 'none', 'eager', 'normal'
  chrome_options.add_argument('--headless')
  chrome_options.add_argument('--no-sandbox')
  chrome_options.add_argument('--disable-dev-shm-usage')
  chrome_options.add_argument('--disable-gpu')
  chrome_options.add_argument(f'user-agent={USER_AGENT}')
  # Chrome 드라이버 설정
  service = Service()
  wd = webdriver.Chrome(service=service, options=chrome_options)
  return wd

websites = [item[2] for item in data]
bing_search_urls1 = websites[:-5]
bing_search_urls2 = websites[-5:]

bing_urls = []
websites = bing_search_urls1
for website in websites:
    search_query = f'site:{website}'
    bing_url = f'https://www.bing.com/search?q={search_query}&setmkt=ID-ID&setlang=id&setloc=ID'
    bing_urls.append(bing_url)

# 오늘자 URL
new_bing_urls = []
for bing_url in bing_urls:
    # ? 다음의 파라미터 부분 추출
    params_start = bing_url.find('?') + 1
    params = bing_url[params_start:]

    # 각 파라미터 추출
    param_list = params.split('&')

    # filters 파라미터 추가 또는 수정 (24시간 필터 적용)
    # 기존에 filters 파라미터가 있는지 확인하고, 있으면 값을 수정하거나 없으면 추가
    filters_param = 'filters=ex1%3a%22ez1%22' #24시간
    #filters_param = 'filters=ex1%3a%22ez2%22' #1주일
    param_list = [param if 'filters=' not in param else filters_param for param in param_list]
    if not any('filters=' in param for param in param_list):
        param_list.append(filters_param)

    # FORM 파라미터 추가 (이미 있는 경우에는 추가하지 않음)
    if not any('FORM=' in param for param in param_list):
        param_list.append('FORM=000017')

    # 변경된 파라미터를 &로 연결하여 새로운 bing_url 생성
    new_bing_url = "https://www.bing.com/search?" + '&'.join(param_list)
    new_bing_urls.append(new_bing_url)

# Initialize an empty list to store dictionaries
articles = []
error_list = []
links_all = []  # 리스트 추가

for index, new_bing_url in enumerate(new_bing_urls):
    all_links = []
    # 페이지를 넘어가며 결과의 모든 링크 수집
    for page in range(50):  # 최대 50 페이지까지만 수집 (원하는 페이지 수로 수정 가능)
        wd = initialize_chrome_driver()
        wd.get(new_bing_url + f'&first={page * 10}')  # 페이지 번호에 따라 파라미터 추가
        html = wd.page_source
        soup = BeautifulSoup(html, 'html.parser')
        if page == 0:
            if soup.find('span', class_='sb_count'):
                span_text = soup.find('span', class_='sb_count').text
                # 숫자에서 쉼표 제거
                span_text_without_comma = re.sub(r',', '', span_text)
                # 정규 표현식을 사용하여 숫자 추출
                match = re.search(r'Sekitar (\d+) hasil', span_text_without_comma)
                result = int(match.group(1)) if match else 0

            else:
                result = 0

        news_items = soup.find_all('li', class_='b_algo')

        for news_item in news_items:
            title = news_item.select_one('h2 a').text
            link = news_item.select_one('h2 a')['href']
            # 기사 링크를 links 리스트에 추가
            links_all.append(link)
            all_links.append(link)

            news_dict = {
                'Title': title,
                'Link': link,
                'Content(RAW)': None  # 나중에 채워질 내용
            }

            articles.append(news_dict)

        # "다음 페이지" 버튼이 없으면 종료
        if not soup.find('a', class_='sb_pagN'):
            break

for i, link in enumerate(links_all):
    try:
        # PDF 파일인 경우 예외 처리
        if link.endswith('.pdf'):
            raise ValueError("PDF file is not supported.")

        article = Article(link, language='id')

        # 기사 다운로드 및 파싱
        article.download()
        article.parse()

        # 기사의 제목, 날짜 및 본문 추출
        text = article.text

        # 해당 기사의 'Content' 열에 추출한 내용 추가
        articles[i]['Content(RAW)'] = text

    except ValueError as ve:
        # 해당 기사의 'Content' 열에 None 추가
        articles[i]['Content(RAW)'] = None

    except Exception as e:
        # 해당 기사의 'Content' 열에 None 추가
        articles[i]['Content(RAW)'] = None

# Create a DataFrame from the list of dictionaries
news_df_gov = pd.DataFrame(articles)
# Remove rows where 'Content(Raw)' is None or an empty string
news_df_gov = news_df_gov[news_df_gov['Content(RAW)'].notna() & (news_df_gov['Content(RAW)'] != '')]
# Reset the index
news_df_gov.reset_index(drop=True, inplace=True)
news_df_gov = news_df_gov.drop_duplicates(subset=['Title'])

news_df_gov.to_csv('IN_Articles_GOV' + datetime.now().strftime("_%y%m%d") + '.csv')

articles_local = []

websites = bing_search_urls2
bing_urls = []
for website in websites:
    search_query = f'site:{website}'
    bing_url = f'https://www.bing.com/search?q={search_query}&setmkt=ID-ID&setlang=id&setloc=ID'
    bing_urls.append(bing_url)

# 오늘자 URL
new_bing_urls = []
for bing_url in bing_urls:
    # ? 다음의 파라미터 부분 추출
    params_start = bing_url.find('?') + 1
    params = bing_url[params_start:]

    # 각 파라미터 추출
    param_list = params.split('&')

    # filters 파라미터 추가 또는 수정 (24시간 필터 적용)
    # 기존에 filters 파라미터가 있는지 확인하고, 있으면 값을 수정하거나 없으면 추가
    filters_param = 'filters=ex1%3a%22ez1%22' #24시간
    #filters_param = 'filters=ex1%3a%22ez2%22' #1주일
    param_list = [param if 'filters=' not in param else filters_param for param in param_list]
    if not any('filters=' in param for param in param_list):
        param_list.append(filters_param)

    # FORM 파라미터 추가 (이미 있는 경우에는 추가하지 않음)
    if not any('FORM=' in param for param in param_list):
        param_list.append('FORM=000017')

    # 변경된 파라미터를 &로 연결하여 새로운 bing_url 생성
    new_bing_url = "https://www.bing.com/search?" + '&'.join(param_list)
    new_bing_urls.append(new_bing_url)

# Initialize an empty list to store dictionaries
links_all = []  # 리스트 추가

for index, new_bing_url in enumerate(new_bing_urls):
    all_links = []
    # 페이지를 넘어가며 결과의 모든 링크 수집
    for page in range(50):  # 최대 50 페이지까지만 수집 (원하는 페이지 수로 수정 가능)
        wd = initialize_chrome_driver()
        wd.get(new_bing_url + f'&first={page * 10}')  # 페이지 번호에 따라 파라미터 추가
        html = wd.page_source
        soup = BeautifulSoup(html, 'html.parser')
        if page == 0:
            if soup.find('span', class_='sb_count'):
                span_text = soup.find('span', class_='sb_count').text
                # 숫자에서 쉼표 제거
                span_text_without_comma = re.sub(r',', '', span_text)
                # 정규 표현식을 사용하여 숫자 추출
                match = re.search(r'Sekitar (\d+) hasil', span_text_without_comma)
                result = int(match.group(1)) if match else 0

            else:
                result = 0

        news_items = soup.find_all('li', class_='b_algo')

        for news_item in news_items:
            title = news_item.select_one('h2 a').text
            link = news_item.select_one('h2 a')['href']
            # 기사 링크를 links 리스트에 추가
            links_all.append(link)
            all_links.append(link)

            news_dict = {
                'Title': title,
                'Link': link,
                'Content(RAW)': None  # 나중에 채워질 내용
            }

            articles_local.append(news_dict)

        # "다음 페이지" 버튼이 없으면 종료
        if not soup.find('a', class_='sb_pagN'):
            break

for i, link in enumerate(links_all):
    try:
        # PDF 파일인 경우 예외 처리
        if link.endswith('.pdf'):
            raise ValueError("PDF file is not supported.")

        article = Article(link, language='id')

        # 기사 다운로드 및 파싱
        article.download()
        article.parse()

        # 기사의 제목, 날짜 및 본문 추출
        text = article.text

        # 해당 기사의 'Content' 열에 추출한 내용 추가
        articles_local[i]['Content(RAW)'] = text

    except ValueError as ve:
        # 해당 기사의 'Content' 열에 None 추가
        articles_local[i]['Content(RAW)'] = None

    except Exception as e:
        # 해당 기사의 'Content' 열에 None 추가
        articles_local[i]['Content(RAW)'] = None


# Create a DataFrame from the list of dictionaries
news_df_local = pd.DataFrame(articles_local)
# Remove rows where 'Content(Raw)' is None or an empty string
news_df_local = news_df_local[news_df_local['Content(RAW)'].notna() & (news_df_local['Content(RAW)'] != '')]
# Reset the index
news_df_local.reset_index(drop=True, inplace=True)
news_df_local = news_df_local.drop_duplicates(subset=['Title'])

news_df_local.to_csv('IN_Articles_LOCAL' + datetime.now().strftime("_%y%m%d") + '.csv')

error_list_df = pd.DataFrame(error_list)
error_list_df.to_csv('IN_Error List' + datetime.now().strftime("_%y%m%d") + '.csv')

# 검색할 기업 리스트
In_Top30_Name_list = [
    "HANA", "GS", "LG", "SK", "DAELIM", "DAEWOO", "DOOSAN",
    "LOTTE", "SAMSUNG", "SHINHAN", "SSANGYONG", "GOLFZON", "Pantos",
    "IBK", "KT&G", "POSCO", "HANHWA", "HANJIN", "HYUNDAI", "KOLON",
    "ASIANA AIRLINE", "KOREAN AIR", "LS", "KUMKANG", "CJ" , "lock&lock", "hansoll"
]

Articles_in_gov_Pattern = {'Company':[],'Title':[],'Link':[],'Content(RAW)':[]}
Articles_in_local_Pattern = {'Company':[],'Title':[],'Link':[],'Content(RAW)':[]}

for index, row in news_df_gov.iterrows():
    # 특수 문자 제거
    title_clean = re.sub(r'[^\w\s]', '', row['Title'])
    content_clean = re.sub(r'[^\w\s]', '', row['Content(RAW)'])
    for company in In_Top30_Name_list:
        # 정규 표현식을 사용하여 정확한 패턴 검색
        pattern = re.compile(r'(?<!\w)' + re.escape(company) + r'(?!\w)')
        if (pattern.search(title_clean)) or (pattern.search(content_clean)):
            Articles_in_gov_Pattern['Company'].append(company)
            Articles_in_gov_Pattern['Title'].append(row['Title'])
            Articles_in_gov_Pattern['Link'].append(row['Link'])
            Articles_in_gov_Pattern['Content(RAW)'].append(row['Content(RAW)'])
            break

for index, row in news_df_local.iterrows():
    # 특수 문자 제거
    title_clean = re.sub(r'[^\w\s]', '', row['Title'])
    content_clean = re.sub(r'[^\w\s]', '', row['Content(RAW)'])
    for company in In_Top30_Name_list:
        # 정규 표현식을 사용하여 정확한 패턴 검색
        pattern = re.compile(r'(?<!\w)' + re.escape(company) + r'(?!\w)')
        if (pattern.search(title_clean)) or (pattern.search(content_clean)):
            Articles_in_local_Pattern['Company'].append(company)
            Articles_in_local_Pattern['Title'].append(row['Title'])
            Articles_in_local_Pattern['Link'].append(row['Link'])
            Articles_in_local_Pattern['Content(RAW)'].append(row['Content(RAW)'])
            break

# 결과를 데이터프레임으로 변환
Articles_Select2_gov_df = pd.DataFrame(Articles_in_gov_Pattern)
Articles_Select2_local_df = pd.DataFrame(Articles_in_local_Pattern)

########################################### <Select 3 : OpenAI> ##############################################
import openai

OPENAI_API_KEY = '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'

# openai API 키 인증
openai.api_key = OPENAI_API_KEY

# ChatGPT - 3.5 turbo updated
def get_completion(prompt, model="gpt-3.5-turbo-1106"):
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message["content"]

Articles_Final_gov = {'Company':[], 'Title':[],'Link':[],'Content(RAW)':[], 'Content(EN)':[]}
Articles_Final_local = {'Company':[], 'Title':[],'Link':[],'Content(RAW)':[], 'Content(EN)':[]}
Articles_Final = {'Company':[], 'Title':[],'Link':[],'Content(RAW)':[], 'Content(EN)':[]}

def translate_with_gpt(text):
    # Use GPT-3 to perform translation
    response = openai.Completion.create(
        engine="text-davinci-003",  # You can choose a different engine
        prompt=f"Translate the following Vietnamese text to English: '{text}'",
        max_tokens=1000  # Adjust as needed
    )

    translation = response.choices[0].text.strip()
    return translation

def translate_long_text(text):
    # Split the text into chunks (adjust chunk_size as needed)
    chunk_size = 500
    text_chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

    # Translate each chunk and concatenate the results
    translated_chunks = [translate_with_gpt(chunk) for chunk in text_chunks]
    translated_text = " ".join(translated_chunks)

    return translated_text

for i in range(len(Articles_Select2_gov_df)):
    company = Articles_Select2_gov_df['Company'][i]
    title = Articles_Select2_gov_df['Title'][i]
    link = Articles_Select2_gov_df['Link'][i]
    content_raw = Articles_Select2_gov_df['Content(RAW)'][i]

    prompt = f"""Article Title = {title} //
            Article Contents = {content_raw} //
            Company Name to Check = {company} //

            Based on the article titled '{title}' and its content,
            please analyze whether the term '{company}' refers to an actual company.
            If '[{company}' is related to a real company, output 'O'.
            If it is not related to a real company, output 'X'.
            """
    response = get_completion(prompt)

    if response == 'O':
        Articles_Final_gov['Company'].append(company)
        Articles_Final_gov['Title'].append(title)
        Articles_Final_gov['Link'].append(link)
        Articles_Final_gov['Content(RAW)'].append(content_raw)

        Articles_Final['Company'].append(company)
        Articles_Final['Title'].append(title)
        Articles_Final['Link'].append(link)
        Articles_Final['Content(RAW)'].append(content_raw)

        # Use GPT for translation
        content_en = translate_long_text(content_raw)

        Articles_Final_gov['Content(EN)'].append(content_en)
        Articles_Final['Content(EN)'].append(content_en)

for i in range(len(Articles_Select2_local_df)):
    company = Articles_Select2_local_df['Company'][i]
    title = Articles_Select2_local_df['Title'][i]
    link = Articles_Select2_local_df['Link'][i]
    content_raw = Articles_Select2_local_df['Content(RAW)'][i]

    prompt = f"""Article Title = {title} //
            Article Contents = {content_raw} //
            Company Name to Check = {company} //

            Based on the article titled '{title}' and its content,
            please analyze whether the term '{company}' refers to an actual company.
            If '[{company}' is related to a real company, output 'O'.
            If it is not related to a real company, output 'X'.
            """
    response = get_completion(prompt)

    if response == 'O':
        Articles_Final_local['Company'].append(company)
        Articles_Final_local['Title'].append(title)
        Articles_Final_local['Link'].append(link)
        Articles_Final_local['Content(RAW)'].append(content_raw)

        Articles_Final['Company'].append(company)
        Articles_Final['Title'].append(title)
        Articles_Final['Link'].append(link)
        Articles_Final['Content(RAW)'].append(content_raw)

        # Use GPT for translation
        content_en = translate_long_text(content_raw)

        Articles_Final_local['Content(EN)'].append(content_en)
        Articles_Final['Content(EN)'].append(content_en)

Articles_Final_gov_df = pd.DataFrame(Articles_Final_gov)
Articles_Final_local_df = pd.DataFrame(Articles_Final_local)
Articles_Final_gov_df = Articles_Final_gov_df.drop_duplicates(subset=['Title'])
Articles_Final_local_df = Articles_Final_local_df.drop_duplicates(subset=['Title'])
Articles_Final_gov_df.to_csv('IN_Articles_GOV_TRANS' + datetime.now().strftime("_%y%m%d") + '.csv')
Articles_Final_local_df.to_csv('IN_Articles_LOCAL_TRANS' + datetime.now().strftime("_%y%m%d") + '.csv')


Articles_Final_df = pd.DataFrame(Articles_Final)
Articles_Final_df = Articles_Final_df.drop_duplicates(subset=['Title'])
Articles_Final_df.to_csv('IN_Final Articles' + datetime.now().strftime("_%y%m%d") + '.csv')